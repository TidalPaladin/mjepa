# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

MJEPA is a self-supervised pre-training library implementing Masked Joint Embedding Predictive Architecture (JEPA) for Vision Transformers. It uses a student-teacher framework with EMA updates, masked prediction, and contrastive losses.

## Build & Development Commands

```bash
make init              # Install dependencies via uv sync --all-groups
make style             # Format code with ruff (lint fixes + formatting)
make quality           # Check linting and formatting without modification
make types             # Run basedpyright type checking
make test              # Run pytest with coverage
make test-<pattern>    # Run tests matching pattern (e.g., make test-model)
make test-pdb-<pattern> # Run tests with PDB debugger
make test-ci           # CI tests without CUDA, writes coverage.xml
```

## Code Style

- Python 3.11â€“3.13; 4-space indentation
- Black line length: 120 characters
- `snake_case` for modules/functions/variables, `CapWords` for classes
- Comprehensive type hints using Python 3.11+ union syntax (`X | None`)

## Testing

- Tests in `tests/` using pytest + pytest-cov
- Markers: `ci_skip` (skip in CI), `cuda` (requires CUDA)
- Check `conftest.py` for fixtures and CUDA detection

## Architecture

**Core modules in `mjepa/`:**

- **model.py**: `MJEPA` class - main module combining student/teacher/predictor networks. Returns `MJEPAPredictions` dataclass from forward pass
- **jepa.py**: Core JEPA components:
  - `CrossAttentionPredictor`: Predicts targets from context using cross-attention
  - `generate_masks()`: Creates non-overlapping context/target masks
  - `update_teacher()`: EMA-based teacher model updates
  - `compute_gram_loss()`: Gram matrix contrastive loss (torch.compile optimized)
  - `compute_sigreg_loss()`: LeJEPA SigREG loss for isotropic distribution
  - `JEPAConfig`: Hyperparameter configuration dataclass
- **trainer.py**: Training utilities, checkpoint management, DDP support, `TrainerConfig` and `ResolutionConfig`
- **optimizer.py**: `OptimizerConfig` for AdamW with OneCycleLR/LinearLR schedulers
- **visualization/**: PCA, cosine similarity, normalization, and positional encoding analysis tools

## Key Implementation Details

- All forward passes must execute during training for DDP compatibility
- Use `torch.inference_mode()` for evaluation
- Mixed precision via autocast (bfloat16 default)
- `torch.compile` used for SigREG and Gram loss functions
- Supports RoPE, register tokens, and CLS tokens
- Config files use YAML with custom constructors for JEPAConfig/OptimizerConfig

## Files Not to Edit

- `mjepa/_version.py` - Auto-generated by hatch-vcs from git tags
