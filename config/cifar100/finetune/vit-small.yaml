# Config for fine-tuning a pretrained ViT-S/4 on CIFAR-10.
trainer: !!python/object:mjepa.TrainerConfig
  batch_size: 256
  num_workers: 24 
  num_epochs: 100
  check_val_every_n_epoch: 5
  accumulate_grad_batches: 4

backbone: !!python/object:vit.ViTConfig
  in_channels: 3
  hidden_size: 384
  ffn_hidden_size: 1536
  patch_size: [4, 4]
  img_size: [64, 64]
  depth: 12
  num_attention_heads: 12
  activation: "srelu"
  bias: false
  attention_dropout: 0.0
  hidden_dropout: 0.1
  drop_path_rate: 0.1
  num_register_tokens: 8
  pos_enc: "fourier"
  layer_scale: 0.00001
  heads:
    cls: !!python/object:vit.HeadConfig
      head_type: "linear"
      pool_type: "attentive"
      out_dim: 100
      num_attention_heads: 12

augmentations: !!python/object:mjepa.AugmentationConfig
  mixup_prob: 0.2
  mixup_alpha: 1.0
  use_noise: true
  noise_prob: 0.1
  noise_clip: true
  invert_prob: 0.1
  solarize_prob: 0.1
  solarize_threshold: 0.5
  posterize_prob: 0.1
  posterize_bits: 6

optimizer: !!python/object:mjepa.OptimizerConfig
  optimizer: "adamw"
  lr: 0.00001
  weight_decay: 0.1
  betas: [0.85, 0.95]
  fused: true
  pct_start: 0.10
  base_momentum: 0.85
  max_momentum: 0.95
  div_factor: 10
  final_div_factor: 10000