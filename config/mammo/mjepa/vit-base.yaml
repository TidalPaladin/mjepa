# Config for training ViT-B/16 using MJEPA on mammography.
trainer: !!python/object:mjepa.TrainerConfig
  batch_size: 256
  num_workers: 24 
  num_epochs: 375
  accumulate_grad_batches: 4
  sizes:
    300: !!python/object:mjepa.ResolutionConfig
      size: [512, 384]
      batch_size: 64
    350: !!python/object:mjepa.ResolutionConfig
      size: [1024, 768]
      batch_size: 16

backbone: !!python/object:vit.ViTConfig
  in_channels: 1
  hidden_size: 768
  ffn_hidden_size: 3072
  patch_size: [16, 16]
  img_size: [256, 192]
  depth: 12
  num_attention_heads: 12
  activation: "swiglu"
  mlp_bias: true
  attention_bias: false
  attention_dropout: 0.1
  hidden_dropout: 0.1
  drop_path_rate: 0.1
  num_register_tokens: 4
  pos_enc: "rope"
  layer_scale: 0.00001
  rope_base: 100
  rope_rescale_coords: 2.0
  rope_shift_coords: null
  rope_jitter_coords: null
  rope_normalize_coords: "max"
  heads:
    pool: !!python/object:vit.HeadConfig
      head_type: "linear"
      pool_type: "attentive"
      attention_dropout: 0.1
      hidden_dropout: 0.1
      output_norm: true

augmentations: !!python/object:mjepa.AugmentationConfig
  mixup_prob: 0.2
  mixup_alpha: 1.0
  use_noise: true
  noise_prob: 0.1
  noise_clip: true
  invert_prob: 0.5
  solarize_prob: 0.1
  solarize_threshold: 0.5
  posterize_prob: 0.1
  posterize_bits: 6

jepa: !!python/object:mjepa.JEPAConfig
  momentum: 0.98
  predictor_depth: 4
  context_ratio: 0.5
  target_ratio: 0.25
  scale: 4
  gram_teacher_epoch: 75
  gram_start_epoch: 125
  gram_update_interval_epoch: 25
  gram_resolution_scale: 2.0
  gram_remove_neg: false
  scheduled: false

optimizer: !!python/object:mjepa.OptimizerConfig
  optimizer: "adamw"
  lr: 0.0002
  weight_decay: 0.1
  betas: [0.85, 0.95]
  fused: true
  scheduled: false
  pct_start: 0.01
  div_factor: 10